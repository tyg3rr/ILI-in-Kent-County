{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training & Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "# ======================================================\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from helper_functions import *\n",
    "import math\n",
    "\n",
    "# Statistics\n",
    "# ======================================================\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.ForecasterSarimax import ForecasterSarimax\n",
    "from skforecast.Sarimax import Sarimax\n",
    "from skforecast.model_selection import grid_search_forecaster\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "from skforecast.model_selection_sarimax import backtesting_sarimax, grid_search_sarimax\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "from skforecast.model_selection import bayesian_search_forecaster\n",
    "from skforecast.ForecasterBaseline import ForecasterEquivalentDate\n",
    "\n",
    "# Warnings Config\n",
    "# ======================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates train      : 2005-01-16 00:00:00 --- 2015-12-27 00:00:00  (n=572)\n",
      "Dates validacion : 2016-01-03 00:00:00 --- 2017-12-31 00:00:00  (n=105)\n",
      "Dates test       : 2017-12-31 00:00:00 --- 2019-12-29 00:00:00  (n=105)\n"
     ]
    }
   ],
   "source": [
    "# Loading & Manipulating Dataset\n",
    "# ======================================================\n",
    "\n",
    "df = fetch_preprocess_dataset()\n",
    "\n",
    "variables = ['GS_cold', 'GS_cough', 'GS_fever', 'GS_flu', \n",
    "            'AWND', 'PRCP','SNOW', 'TAVG','TMAX', 'TMIN',\n",
    "            'Overall AQI Value', \n",
    "            'CO', 'Ozone', 'PM10', 'PM25', 'Days Moderate', \n",
    "            'Days Unhealthy', 'visits','Main Pollutant_CO', \n",
    "            'Main Pollutant_NO2', 'Main Pollutant_PM2.5']\n",
    "\n",
    "# adding lagged variables up to three weeks\n",
    "# to capture potential lagged effects\n",
    "for v in variables:\n",
    "    for i in range(1,4):\n",
    "        df[f'{v}_L{i}'] = df[v].shift(i)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "end_train, end_validation, df_train, df_val, df_test = train_test_validate_split(\n",
    "                                                        df, end_train = date(2015,12,31), \n",
    "                                                        end_validation = date(2017,12,31)\n",
    "                                                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 196.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation & fit\n",
    "# ======================================================\n",
    "\n",
    "forecaster1 = ForecasterAutoreg(\n",
    "    regressor=ElasticNetCV(\n",
    "        verbose=False,\n",
    "        random_state=123,\n",
    "        l1_ratio=.5,\n",
    "        selection='cyclic'\n",
    "    ),\n",
    "    lags=[1,2,52],\n",
    "    transformer_y=FunctionTransformer(func = np.log1p, inverse_func = np.expm1, validate=True),\n",
    "    transformer_exog=StandardScaler())\n",
    "\n",
    "forecaster1.fit(y=df_train['cases'],exog=df_train.drop(columns=['cases']))\n",
    "\n",
    "# Model optimization on validation dataset\n",
    "# ======================================================\n",
    "\n",
    "met, preds = backtesting_forecaster(\n",
    "    forecaster = forecaster1,\n",
    "    y = df['cases'][:end_validation],\n",
    "    exog = df.drop(columns=['cases'])[:end_validation],\n",
    "    initial_train_size = len(df_train),\n",
    "    steps = 2,\n",
    "    metric = 'mean_absolute_error',\n",
    "    refit = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 70.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lags grid: 100%|██████████| 7/7 [00:27<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1 52] \n",
      "  Parameters: {'l1_ratio': 0.1, 'selection': 'random'}\n",
      "  Backtesting metric: 104937.6564688119\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 196.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216.4302285042319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning via grid search\n",
    "# ======================================================\n",
    "\n",
    "lags_grid = [[52],[1],[1,52],[1,2],[1,2,52],[1,2,3],[1,2,3,52]]\n",
    "param_grid = {\n",
    "    'l1_ratio':[.001, .05, 0.10, 0.50, 0.70],\n",
    "    'selection':['cyclic','random']\n",
    "}\n",
    "\n",
    "results_grid = grid_search_forecaster(\n",
    "    forecaster = forecaster1,\n",
    "    y = df['cases'][:end_validation],\n",
    "    exog = df.drop(columns=['cases'])[:end_validation],\n",
    "    param_grid=param_grid,\n",
    "    lags_grid=lags_grid,\n",
    "    steps=2,\n",
    "    refit=False,\n",
    "    metric='mean_squared_error',\n",
    "    initial_train_size=len(df_train),\n",
    "    fixed_train_size=True,\n",
    "    return_best=True,\n",
    "    verbose=False,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "met, preds = backtesting_forecaster(\n",
    "    forecaster = forecaster1,\n",
    "    y = df['cases'][:end_validation],\n",
    "    exog = df.drop(columns=['cases'])[:end_validation],\n",
    "    initial_train_size = len(df_train),\n",
    "    steps = 2,\n",
    "    metric = 'mean_absolute_error',\n",
    "    refit = False)\n",
    "\n",
    "print(met)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary modeling for feature selection\n",
    "# ======================================================\n",
    "\n",
    "exog = pd.DataFrame(\n",
    "        StandardScaler()\\\n",
    "                .fit_transform(df_train.drop(columns=['cases'])),    # scaling exog features\n",
    "                columns = df_train.drop(columns=['cases']).columns,\n",
    "                index = df_train.index\n",
    "                )\n",
    "\n",
    "sarimaxmod2 = SARIMAX(\n",
    "                        endog = df_train['cases'].apply(np.log1p),\n",
    "                        exog = exog.drop(columns=['epiweek','Days Good']),  # prelim. sarimax\n",
    "                        order = (1, 1, 1),\n",
    "                        seasonal_order = (0, 1, 0, 52)\n",
    "                        )\n",
    "\n",
    "sarimaxres2 = sarimaxmod2.fit()\n",
    "sarimax_exogs = sarimaxres2.params\\\n",
    "                .loc[sarimaxres2.params>=.05]\\\n",
    "                .index.to_list()[:-2]               # selecting features with sufficiently large params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "# ======================================================\n",
    "\n",
    "forecaster2 = ForecasterSarimax(\n",
    "    regressor=Sarimax(\n",
    "        order=(1, 1, 1),\n",
    "        seasonal_order=(0, 1, 0, 52),\n",
    "        maxiter=100\n",
    "    ),\n",
    "    transformer_y=FunctionTransformer(func = np.log1p, inverse_func = np.expm1, validate=True),\n",
    "    transformer_exog=StandardScaler())\n",
    "\n",
    "forecaster2.fit(y=df_train['cases'],exog=df_train[sarimax_exogs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for hyperparameter tuning\n",
    "# note that this grid search took over 6 hours\n",
    "# ======================================================\n",
    "\n",
    "param_grid = {\n",
    "    'order': [(0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (2, 1, 1)],\n",
    "    'seasonal_order': [(0, 0, 0, 0), (0, 1, 0, 52), (1, 1, 1, 52)],\n",
    "    'trend': [None, 'n', 'c']\n",
    "}\n",
    "\n",
    "results_grid = grid_search_sarimax(\n",
    "                forecaster            = forecaster2,\n",
    "                y                     = df['cases'].loc[:end_validation],\n",
    "                param_grid            = param_grid,\n",
    "                steps                 = 2,\n",
    "                refit                 = False,\n",
    "                metric                = 'mean_absolute_error',\n",
    "                initial_train_size    = len(df_train),\n",
    "                fixed_train_size      = False,\n",
    "                return_best           = False,\n",
    "                n_jobs                = 'auto',\n",
    "                suppress_warnings_fit = True,\n",
    "                verbose               = False,\n",
    "                show_progress         = True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit with new hyperparameters\n",
    "# ======================================================\n",
    "\n",
    "forecaster3 = ForecasterSarimax(\n",
    "    regressor=Sarimax(\n",
    "        order=(1, 1, 1),\n",
    "        seasonal_order=(0, 1, 0, 52),\n",
    "        maxiter=100,\n",
    "        trend='c'\n",
    "    ),\n",
    "    transformer_y=FunctionTransformer(func = np.log1p, inverse_func = np.expm1, validate=True),\n",
    "    transformer_exog=StandardScaler())\n",
    "\n",
    "forecaster3.fit(y=df_train['cases'],exog=df_train[sarimax_exogs+['epiweek_sin','epiweek_cos','epiweek']])\n",
    "\n",
    "# Model optimization on validation dataset\n",
    "# ======================================================\n",
    "\n",
    "met3, preds3 = backtesting_sarimax(\n",
    "    forecaster = forecaster3,\n",
    "    y = df['cases'][:end_validation],\n",
    "    exog = df[sarimax_exogs+['epiweek_sin','epiweek_cos','epiweek']][:end_validation],\n",
    "    initial_train_size = len(df_train),\n",
    "    steps = 2,\n",
    "    metric = 'mean_absolute_error',\n",
    "    refit = False,\n",
    "    verbose = False,\n",
    "    show_progress=True,\n",
    "    suppress_warnings_fit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation \n",
    "# ======================================================\n",
    "\n",
    "forecaster4 = ForecasterAutoreg(\n",
    "    regressor=RandomForestRegressor(\n",
    "        random_state=123,\n",
    "        n_estimators=400,\n",
    "        max_depth=10\n",
    "    ),\n",
    "    lags = [1,2,52],\n",
    "    transformer_y=FunctionTransformer(func = np.log1p, inverse_func = np.expm1, validate=True),\n",
    "    transformer_exog=StandardScaler())\n",
    "\n",
    "forecaster4.fit(y=df_train['cases'],exog=df_train.drop(columns=['cases']))\n",
    "\n",
    "# Optimizing on validation dataset\n",
    "# ======================================================\n",
    "\n",
    "met4, preds4 = backtesting_forecaster(\n",
    "    forecaster=forecaster4,\n",
    "    y=df['cases'][:end_validation],\n",
    "    exog=df.drop(columns=['cases'])[:end_validation],\n",
    "    initial_train_size=len(df_train),\n",
    "    steps = 2,\n",
    "    metric='mean_absolute_error',\n",
    "    refit=False,\n",
    "    verbose=False,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive feature elimination with cross-validation \n",
    "# ======================================================\n",
    "\n",
    "X_train, y_train = forecaster4.create_train_X_y(\n",
    "                    y    = df_train['cases'],\n",
    "                    exog = df_train.drop(columns=['cases'])\n",
    "                )\n",
    "\n",
    "rng = np.random.default_rng(seed=785412)\n",
    "sample = rng.choice(X_train.index, size=int(len(X_train)*0.5), replace=False)\n",
    "X_train_sample = X_train.loc[sample, :]\n",
    "y_train_sample = y_train.loc[sample]\n",
    "\n",
    "regressor = RandomForestRegressor(random_state=123)\n",
    "\n",
    "selector = RFECV(\n",
    "                estimator              = regressor,\n",
    "                step                   = 1,\n",
    "                cv                     = 3,\n",
    "                min_features_to_select = 15,\n",
    "                n_jobs                 = -1\n",
    "                )\n",
    "\n",
    "selector.fit(X_train_sample, y_train_sample)\n",
    "selected_features_rfe = selector.get_feature_names_out()\n",
    "\n",
    "selected_exog_features = [\n",
    "    feature\n",
    "    for feature in selected_features_rfe\n",
    "    if not feature.startswith(\"lag_\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit with selected features \n",
    "# ======================================================\n",
    "\n",
    "forecaster5 = ForecasterAutoreg(\n",
    "    regressor=RandomForestRegressor(random_state=123,\n",
    "        n_estimators=400,\n",
    "        max_depth=10),\n",
    "    lags=[1,2,52],\n",
    "    transformer_y=FunctionTransformer(func = np.log1p, inverse_func = np.expm1, validate=True),\n",
    "    transformer_exog=StandardScaler())\n",
    "\n",
    "forecaster5.fit(y=df_train['cases'],exog=df_train[selected_exog_features])\n",
    "\n",
    "# Optimize on validation dataset \n",
    "# ======================================================\n",
    "\n",
    "met5, preds5 = backtesting_forecaster(\n",
    "    forecaster=forecaster5,\n",
    "    y=df['cases'][:end_validation],\n",
    "    exog=df[selected_exog_features][:end_validation],\n",
    "    initial_train_size=len(df_train),\n",
    "    steps = 2,\n",
    "    metric='mean_absolute_error',\n",
    "    refit=False,\n",
    "    verbose=False,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Seasonal Naive\n",
    "**Baseline model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instanciation \n",
    "# ======================================================\n",
    "\n",
    "baselineforecaster = ForecasterEquivalentDate(\n",
    "                                                offset=52,\n",
    "                                                n_offsets=2,\n",
    "                                                agg_func=np.mean\n",
    "                                            )\n",
    "\n",
    "baselineforecaster.fit(df_train['cases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing with validation dataset\n",
    "# ======================================================\n",
    "\n",
    "mb1, baseline_preds1 = backtesting_forecaster(\n",
    "    forecaster=baselineforecaster,\n",
    "    y=df['cases'],\n",
    "    initial_train_size=len(df_train),\n",
    "    refit=True,\n",
    "    steps=1,\n",
    "    verbose=False,\n",
    "    metric='mean_absolute_error',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skforecast.utils import save_forecaster, load_forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_forecaster(forecaster1, file_name='ElasticNet_001 hide.py',verbose=False)\n",
    "save_forecaster(forecaster3, file_name='SARIMAX_001 hide.py',verbose=False)\n",
    "save_forecaster(forecaster5, file_name='RandomForest_001 hide.py',verbose=False)\n",
    "save_forecaster(baselineforecaster, file_name='baseline_001 hide.py',verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
